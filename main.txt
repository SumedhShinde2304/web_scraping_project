import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd

college_details = []

def get_college_info(num_of_scroll):
    # Configure Chrome options
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without a visible browser window)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    chrome_options.add_argument(f'user-agent={headers["User-Agent"]}')

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Navigate to the website
    base_url = 'https://collegedunia.com/india-colleges'
    driver.get(base_url)

    for _ in range(num_of_scroll):
        print("scrolling")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.5)  # Adjust the sleep duration as needed


    # Get the updated page source after scrolling
    page_source = driver.page_source

    # Close the WebDriver
    driver.quit()
    # Parse the updated page source with BeautifulSoup
    soup = BeautifulSoup(page_source, 'html.parser')
    college_divs = soup.find_all('tbody', class_='jsx-2796823646')

    for college_div in college_divs:
        for i in college_div:
            link = i.find('div',class_='clg-name-address').find('a')['href']
            name = i.find('div',class_='clg-name-address').find('h3').text.strip()
            location = i.find('div',class_='clg-name-address').find('span',class_='location').text.strip().split(',')
            city = location[0]
            state = location[1]

            approvals_span = i.find('div', class_='clg-name-address').find('span', class_='approvals')
            approvals = approvals_span.text.replace('\xa0', ' ').strip() if approvals_span is not None else 'NA'
            course_fees = i.find('td',class_='col-fees').find('span').text.strip()
            courses_list=i.find('td',class_='col-fees').find_all('span')
            course_name=courses_list[2].text.strip()
            # import pdb;pdb.set_trace()
            # user_reviews =i.find('td',class_='col-reviews')
            
            # Check if the placement section exists
            placement_td = i.find('td', class_='col-placement')
            if placement_td:
                # Find all placement spans
                placements = placement_td.find_all('span', class_='jsx-914129990')

                # Check if placements is not None and has at least 3 elements
                if placements and len(placements) >= 3:
                    avg_placement = placements[0].text.replace('\xa0', '').strip()
                    highest_placement = placements[2].text.replace('\xa0', '').strip()
                else:
                    avg_placement = 'NA'
                    highest_placement = placements[0].text.replace('\xa0', '').strip()
            else:
                # Handle the case when there is no placement section
                avg_placement = 'NA'
                highest_placement = 'NA'
                
                

            college_details.append({
                'Link': link,
                'College Name': name,
                'City': city,
                'State': state,
                'Approvals': approvals,
                'Course Fees': course_fees,
                'Course Name':course_name,
                # 'User Riviews': user_reviews,
                'Average Placement': avg_placement,
                'Highest Placement': highest_placement,
            })

    # Create a DataFrame from the college details list
    df = pd.DataFrame(college_details)

    # Save the DataFrame to a JSON file
    df.to_json('practice_details2.json', orient='records', indent=2)
    print("JSON file created successfully.")

    # Save the DataFrame to an Excel file
    df.to_excel('practice_details2.xlsx', index=False)
    print("Excel file created successfully.")

get_college_info(3)
